{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Изложение построено по [этому](https://www.youtube.com/watch?v=kCc8FmEb1nY&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&index=7) материалу, спасибо Andrej Karpathy за то, что поделился своим опытом с миром"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попытаемся за разумное время построить модель, которая будет генерировать связный текст.\n",
    "\n",
    "## Составим словарь\n",
    "В этот раз будем работать на уровне отдельных символов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего 1115394 символа\n",
      "------------\n",
      "\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "print(f\"Всего {len(text)} символа\\n------------\\n\")\n",
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Нулевой элемент нашего словаря - это '\\n'\n"
     ]
    }
   ],
   "source": [
    "print(f'Нулевой элемент нашего словаря - это {repr(chars[0])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 1, 58, 46, 43, 56, 43]\n",
      "hi there\n"
     ]
    }
   ],
   "source": [
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "print(encode(\"hi there\"))\n",
    "print(decode(encode(\"hi there\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Раз у нас на повестке дня нейросети, будем использовать Pytorch. Превратим наши данные в подходящий ему формат:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала будем оперировать контекcтом размера 8:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Для инпута \"tensor([18])\" целевой символ \"47\"\n",
      "Для инпута \"tensor([18, 47])\" целевой символ \"56\"\n",
      "Для инпута \"tensor([18, 47, 56])\" целевой символ \"57\"\n",
      "Для инпута \"tensor([18, 47, 56, 57])\" целевой символ \"58\"\n",
      "Для инпута \"tensor([18, 47, 56, 57, 58])\" целевой символ \"1\"\n",
      "Для инпута \"tensor([18, 47, 56, 57, 58,  1])\" целевой символ \"15\"\n",
      "Для инпута \"tensor([18, 47, 56, 57, 58,  1, 15])\" целевой символ \"47\"\n",
      "Для инпута \"tensor([18, 47, 56, 57, 58,  1, 15, 47])\" целевой символ \"58\"\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f'Для инпута \"{context}\" целевой символ \"{target}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Инпут:\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "Целевые символы:\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "----\n",
      "Для инпута \"[24]\" целевой символ \"43\"\n",
      "Для инпута \"[24, 43]\" целевой символ \"58\"\n",
      "Для инпута \"[24, 43, 58]\" целевой символ \"5\"\n",
      "Для инпута \"[24, 43, 58, 5]\" целевой символ \"57\"\n",
      "Для инпута \"[24, 43, 58, 5, 57]\" целевой символ \"1\"\n",
      "Для инпута \"[24, 43, 58, 5, 57, 1]\" целевой символ \"46\"\n",
      "Для инпута \"[24, 43, 58, 5, 57, 1, 46]\" целевой символ \"43\"\n",
      "Для инпута \"[24, 43, 58, 5, 57, 1, 46, 43]\" целевой символ \"39\"\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "\n",
    "def get_batch(split, block_size=8):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('Инпут:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('Целевые символы:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('----')\n",
    "\n",
    "for t in range(block_size): # time dimension\n",
    "    context = xb[0, :t+1]\n",
    "    target = yb[0,t]\n",
    "    print(f'Для инпута \"{context.tolist()}\" целевой символ \"{target}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Простая языковая модель\n",
    "\n",
    "Которая работает по тому же принципу, что и word2vec, который мы рассматривали ранее. Для каждого элемента нашего словаря обучаем вектор, а потом используем эти векторы. Только сейчас элементы нашего словаря - символы, а не слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "#!          смотрим только на предыдущий символ!\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "#!          берём не какой-то один самый вероятный символ, а сэмплируем случайный, используя полученные вероятности.\n",
    "#!          в языке работает также есть [чёрный к...]от, а есть [чёрный к...]амаз\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(5.0364, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "l-QYjt'CL?jLDuQcLzy'RIo;'KdhpV\n",
      "vLixa,nswYZwLEPS'ptIZqOZJ$CA$zy-QTkeMk x.gQSFCLg!iW3fO!3DGXAqTsq3pdgq\n",
      "---\n",
      "\n",
      "!LznIeJydZJSrFSrPLR!:VwWSmFNxbjPiNYQ:sry,OfKrxfvJI$WS3JqCbB-TSQXeKroeZfPL&,:opkl;Bvtz$LmOMyDjxxaZWtp\n",
      "---\n",
      "\n",
      "v,OxZQsWZalk'uxajqgoSXAWt'e.Q$.lE-aV\n",
      ";spkRHcpkdot:u'-NGEzkMPy'hZCWhv.w.q!f'mOxF&IDRR,x\n",
      "?$Ox?xj.BHJsG\n"
     ]
    }
   ],
   "source": [
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n",
    "print('---')\n",
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n",
    "print('---')\n",
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попытаемся это обучить!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model,eval_iters=100):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': tensor(3.6930), 'val': tensor(3.7040)}\n",
      "{'train': tensor(3.1094), 'val': tensor(3.1279)}\n",
      "{'train': tensor(2.8065), 'val': tensor(2.8111)}\n",
      "{'train': tensor(2.6512), 'val': tensor(2.6535)}\n",
      "{'train': tensor(2.5492), 'val': tensor(2.5751)}\n",
      "{'train': tensor(2.5202), 'val': tensor(2.5350)}\n",
      "{'train': tensor(2.5023), 'val': tensor(2.5223)}\n",
      "{'train': tensor(2.4875), 'val': tensor(2.5029)}\n",
      "{'train': tensor(2.4660), 'val': tensor(2.4986)}\n",
      "{'train': tensor(2.4801), 'val': tensor(2.4931)}\n",
      "{'train': tensor(2.4444), 'val': tensor(2.4905)}\n",
      "{'train': tensor(2.4615), 'val': tensor(2.4723)}\n",
      "{'train': tensor(2.4635), 'val': tensor(2.4907)}\n",
      "{'train': tensor(2.4479), 'val': tensor(2.5041)}\n",
      "{'train': tensor(2.4632), 'val': tensor(2.4970)}\n",
      "{'train': tensor(2.4638), 'val': tensor(2.4915)}\n",
      "{'train': tensor(2.4548), 'val': tensor(2.4958)}\n",
      "{'train': tensor(2.4424), 'val': tensor(2.4946)}\n",
      "{'train': tensor(2.4533), 'val': tensor(2.4827)}\n",
      "{'train': tensor(2.4488), 'val': tensor(2.4845)}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
    "batch_size = 32\n",
    "for _ in range(20):\n",
    "    for steps in range(1000): # increase number of steps for good results...\n",
    "\n",
    "        # sample a batch of data\n",
    "        xb, yb = get_batch('train')\n",
    "\n",
    "        # evaluate the loss\n",
    "        logits, loss = m(xb, yb)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(estimate_loss(m))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "S faregur t ayorff houndfis cod sws h ovig nd fa his d R aclidipo her d\n",
      "PSALedrs fureauplf tton mags\n",
      "---\n",
      "\n",
      "Tofee, feeay ous?\n",
      "an stheh mis I maky,\n",
      "Whery wanotheliusown anithy g, tom o whoull ou teedow tipino \n",
      "---\n",
      "\n",
      "O, n tr No.\n",
      "IO w sey t\n",
      "Tho Cl s rteastthes mensithiraserenm, u semeyor at ng lcke tr semivengincasth\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n",
    "print('---')\n",
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n",
    "print('---')\n",
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Как сделать модель поумнее?\n",
    "\n",
    "Сейчас мы пытаемся предсказать символ, глядя только на один предыдущий. Кажется, язык устроен чуть сложнее. Давайте попробуем как-нибудь учесть большее число символов нашей последовательности. Например, попробуем их усреднить.\n",
    "\n",
    "Для начала научимся эффективно усреднять столбцы случайной матрицы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,2 # batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# не очень-то эффективно\n",
    "xbow = torch.zeros((B,T,C)) # bow - потому что Bag Of Words. Помните усреднение векторов в Word2Vec, которое так называлось? Вот, термин оттуда.\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b,:t+1] # (t,C)\n",
    "        xbow[b,t] = torch.mean(xprev, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# гораздо лучше\n",
    "wei = torch.tril(torch.ones(T, T))     # нижнетреугольная матрица из единиц\n",
    "wei = wei / wei.sum(1, keepdim=True)   # нормируем строки\n",
    "xbow2 = wei @ x # (B, T, T) @ (B, T, C) ----> (B, T, C)\n",
    "\n",
    "torch.allclose(xbow, xbow2, atol=1e-7) # результат правильный"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Пока непонятно зачем, но тоже работает\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "xbow3 = wei @ x\n",
    "\n",
    "torch.allclose(xbow, xbow3, atol=1e-7) # результат правильный\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделаем из этого что-то полезное:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 32])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32 # batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "# Эта конструкция - то, что называется self-attention\n",
    "head_size = 32\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x)   # (B, T, 16)\n",
    "q = query(x) # (B, T, 16)\n",
    "wei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "v = value(x)\n",
    "out = wei @ v\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00],\n",
       "        [8.3841e-01, 1.6159e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00],\n",
       "        [8.8476e-01, 1.0445e-01, 1.0783e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00],\n",
       "        [9.5338e-02, 6.5157e-01, 1.8913e-01, 6.3963e-02, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00],\n",
       "        [8.6153e-01, 2.7327e-02, 3.9553e-03, 1.6660e-02, 9.0524e-02, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00],\n",
       "        [4.6015e-05, 3.2064e-05, 1.1772e-02, 4.2589e-01, 2.5680e-06, 5.6226e-01,\n",
       "         0.0000e+00, 0.0000e+00],\n",
       "        [5.4061e-02, 6.4071e-02, 2.5209e-01, 8.7270e-02, 2.3189e-01, 2.3952e-01,\n",
       "         7.1099e-02, 0.0000e+00],\n",
       "        [5.4092e-02, 1.0762e-01, 1.6648e-02, 1.6889e-01, 6.2352e-01, 6.3775e-03,\n",
       "         9.1127e-03, 1.3737e-02]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Картинка [отсюда](https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html)\n",
    "\n",
    "<img src=images/summary.png width=1200></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С помощью этого механизма можно учитывать более важные куски с большим весом, а неважные - не учитывать вовсе. Давайте разбираться на примере картинки [отсюда](https://arxiv.org/pdf/1601.06733.pdf)\n",
    "\n",
    "<img src=images/fbi.png width=800></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Важное\n",
    "\n",
    "* Attention - это механизм для коммуникации. Это можно представить как узлы направленного графа, которые обращаются друг к другу и агрегируют информацию с использованием взвешенной суммы от всех связанных узлов, с весами, зависящими от данных.\n",
    "* Никак не учитывается пространства, или порядок символов. Attention просто действует над набором векторов с помощью коммутативных операций. **С этим нужно что-то сделать, т.к. порядок символов для языка важен**\n",
    "* Каждый пример в батче обрабатывается полностью независимо, и они никак не \"общаются\" друг с другом.\n",
    "\n",
    "Попытаемся впилить это в нашу модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size, n_embd, context_size, dropout):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(context_size, context_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,head_size)\n",
    "        q = self.query(x) # (B,T,head_size)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, head_size) @ (B, head_size, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei) #! применяем дропаут\n",
    "        v = self.value(x) # (B,T,head_size)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, head_size) -> (B, T, head_size)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, context_size, head_size, n_embd, dropout):\n",
    "        super().__init__()\n",
    "        self.context_size = context_size\n",
    "        # теперь мы маппим наши символы в пространство размерности n_embd\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "\n",
    "        # Добавим информацию о позициях символов\n",
    "        self.position_embedding_table = nn.Embedding(context_size, n_embd)\n",
    "\n",
    "        self.sa_head = Head(head_size=head_size, n_embd=n_embd,\n",
    "                            context_size=context_size, dropout=dropout)\n",
    "        self.lm_head = nn.Linear(head_size, vocab_size)\n",
    "\n",
    "\n",
    "    def forward(self, idx, targets=None, device='cpu'):\n",
    "        idx = idx.to(device)\n",
    "        if targets is not None:\n",
    "            targets = targets.to(device)\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (B,T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.sa_head(x) # (B,T,C)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # чтобы не переполнились позиционные эмбеддинги, подрежем контекст\n",
    "            idx_cond = idx[:,-self.context_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "#!          смотрим только на предыдущий символ!\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "#!          берём не какой-то один самый вероятный символ, а сэмплируем случайный, используя полученные вероятности.\n",
    "#!          в языке работает также есть [чёрный к...]от, а есть [чёрный к...]амаз\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': tensor(2.4890), 'val': tensor(2.4778)}\n",
      "{'train': tensor(2.4431), 'val': tensor(2.4458)}\n",
      "{'train': tensor(2.4215), 'val': tensor(2.4262)}\n",
      "{'train': tensor(2.4001), 'val': tensor(2.4238)}\n",
      "{'train': tensor(2.3775), 'val': tensor(2.4081)}\n",
      "{'train': tensor(2.3716), 'val': tensor(2.3919)}\n",
      "{'train': tensor(2.3810), 'val': tensor(2.3907)}\n",
      "{'train': tensor(2.3645), 'val': tensor(2.3885)}\n",
      "{'train': tensor(2.3698), 'val': tensor(2.3812)}\n",
      "{'train': tensor(2.3600), 'val': tensor(2.3813)}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "new_context_size = 8\n",
    "model = BigramLanguageModel(vocab_size, context_size=new_context_size, head_size=32, n_embd=32, dropout=0.2)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "batch_size = 32\n",
    "for _ in range(10):\n",
    "    for steps in range(2000): # increase number of steps for good results...\n",
    "\n",
    "        # sample a batch of data\n",
    "        xb, yb = get_batch('train',new_context_size)\n",
    "\n",
    "        # evaluate the loss\n",
    "        logits, loss = model.forward(xb, yb,device='cpu')\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(estimate_loss(model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Te,\n",
      "Wipturdan yr ding,\n",
      "Th hoendde ongt at\n",
      "Alis I St lll ms thit h viloeve  ry--fpe nous torow imt bt\n",
      "---\n",
      "\n",
      "MEL ISCofr ouf, oun sot tito.\n",
      "avemiegre e souse athiletnour, ncdaauscheru ke have, Sons ye me.\n",
      "\n",
      "em\n",
      "W\n",
      "---\n",
      "\n",
      "\n",
      "NGEort wdinr usuts\n",
      "eriche ad azen BwTa b the;\n",
      "o wn ogivindsw ds ne?T:\n",
      "\n",
      "At; or fick, lsunleve ant la\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(decode(model.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n",
    "print('---')\n",
    "print(decode(model.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n",
    "print('---')\n",
    "print(decode(model.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=images/transformer.png width=600></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* В encoder attention просто удаляется одна строка, которая выполняет маскирование с помощью tril, позволяя всем токенам общаться. Если маскирование есть, то блок называется decoder attention.\n",
    "* \"Self-attention\" означает, что ключи и значения производятся из того же источника, что и запросы. В \"перекрёстном внимании\" запросы по-прежнему производятся из x, но ключи и значения поступают из какого-то другого, внешнего источника (например, из модуля кодировщика).\n",
    "* \"Scaled-attention\" дополнительно делит wei на корень квадратный из размерности данных. Это делается для того, чтобы при единичной дисперсии входных Q, K, wei также имела единичную дисперсию, и функция Softmax оставалась распределённой, а не насыщалась слишком сильно, обращаясь в пик. Иллюстрация ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.788929 M parameters\n",
      "step 0: train loss 4.2363, val loss 4.2397\n",
      "\n",
      "SaVqulrf&&;gSYUCZo-TR,DMg\n",
      "qlHHkSkrj!'VTSbmrMynJK.H3KU.nbO,h&MHKHP;\n",
      "qkjno n$ReYPV$!DAyC Dbb$Ib&KIGEfr.Ca-k$orYHaorCZcn\n",
      "owkvu'WP.LvDFxdFh,jI.DINfmSBQzkma\n",
      "Ib,sO-\n",
      "ErmaEkLcQoq'FsxYmshIbPU!-T'srnjwFsqhg.GpckkVSV,KD?R, z?Zyk.D:FB-C3raj.-$JJ-soQ!,$D;;p lfxSsN\n",
      "LQm;b\n",
      "ofDxto&GEN!:cEsnRC?,kHozPuYWJmiUMF;nIH,DxjGPgLwo.3Io$dAl!B?kUJS$KIrqDJzp\n",
      "K!nxWWfYax ,PhtgCAMYn$;?3K!t,z3\n",
      "NQF.yEauUDcZ m MSiFJl-$oFj$xldQDquDsig-.eevDi dm'gokmq$dft,BxmyVwo-r BHMjwI\n",
      "Xyyro; PTumsy-cCUah dMA\n",
      "MirDMATVx$yAkVxh$'w\n",
      "x$!G3SHiM-Ts,&\n",
      "sM\n",
      "-----\n",
      "step 500: train loss 2.1315, val loss 2.2017\n",
      "\n",
      "MUTIUS:\n",
      "GLode if ywill vandd unt an cimant lugdy,\n",
      "Unow the then lupant In woal th thisthey!\n",
      "\n",
      "MENENENIUS:\n",
      "Costate unnd gad the ofor sasto 'sicie,\n",
      "We in to he glio in no wix siced I ta o\n",
      "Wickid durod be! I theee depege fonce for urth all\n",
      "Alind thifer se, land tlame my fall mus lis;\n",
      "Anove is ther dethaar ton o wilcon id,\n",
      "I Pror oablach,\n",
      "aK heme, ast gryef a t loman be sese!-leee compor u to'd\n",
      "Canimi-s treal, andig cete.\n",
      "\n",
      "\n",
      "SLEONTES:\n",
      "Hat you You, ay:\n",
      "As r ould of in Behereces gritationgr.\n",
      "ADIZ\n",
      "\n",
      "WI Co\n",
      "-----\n",
      "step 1000: train loss 1.7727, val loss 1.9193\n",
      "\n",
      "Messenger:\n",
      "thy sentles ber you away\n",
      "Of come, whome, they incousy voices defend\n",
      "To ime time the house may sons, you eye tof\n",
      "our gentleman: but to redittenematic, and you mlust,\n",
      "not whildive here demides nit\n",
      "sephting king me ato wall for the chace and\n",
      "Have won shall marrys the wereds of Sugjest,\n",
      "Boublinge soul, the art yet warr spears be prease.\n",
      "Couse that sivers\n",
      "Mation to spervoners, inness and off tranch\n",
      "unto dearsx: and not\n",
      "beiagl they doy, any himse shill do need make not\n",
      "Stally this beak in t\n",
      "-----\n",
      "step 1500: train loss 1.6941, val loss 1.8419\n",
      "\n",
      "\n",
      "Nurse;\n",
      "If they madam, go not Vonsces.\n",
      "\n",
      "ROMEO:\n",
      "Leave you?\n",
      "What have, we a love?\n",
      "\n",
      "Roman:\n",
      "Holy harging moody?\n",
      "\n",
      "GREY:\n",
      "Nay, sir, here the slaugh?\n",
      "\n",
      "KING RICHARD II:\n",
      "Strike unt; timely I hear thee,\n",
      "And Villourses, make my spoke.\n",
      "\n",
      "GLOUCESTER:\n",
      "\n",
      "GANCE:\n",
      "But, both no more to the poison, whom is honours.\n",
      "\n",
      "Officer:\n",
      "HAS from Bagall gozen of sac\n",
      "Thankful, in let where is eyes and the entrakes\n",
      "Forwarder and men: let break your\n",
      "Thank yours this hags she, I was\n",
      "A Nlord Gaunt bellow nightly.\n",
      "\n",
      "LATEDS:\n",
      "Then thus bed\n",
      "-----\n",
      "step 2000: train loss 1.6529, val loss 1.8277\n",
      "\n",
      "Quice meed, but, root that tyrants, good tonbury:\n",
      "What Rudges to the -country's coats days?\n",
      "With some a little strength hights which glock'd his honing;\n",
      "I do warrant to the quarrel upon throng move;\n",
      "And we'll before be maled it reeabburds:\n",
      "But awaked me impatiends;\n",
      "That I would but mean him like a queen of mine.\n",
      "Though I see thy hunvance to be alterning evence,\n",
      "Thougs yet war as-deam Hourselen.\n",
      "And empane of news rephenced\n",
      "Why breathen'd harms folking, is succed aignan,\n",
      "Daure but the the swords-\n",
      "-----\n",
      "step 2500: train loss 1.6312, val loss 1.8113\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "Can you call the comforcess i' the close.\n",
      "\n",
      "ISABELLA:\n",
      "No, then; but you music debt,\n",
      "If, thou hast pass, most unpost, party towantor, one,\n",
      "As almost that kneedst well in the sind.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "Poor worsetlable looks upon a tongue\n",
      "For disdaying; I conselloured it us now:\n",
      "To runh already soldiers, and I kneel\n",
      "You fgare of the nurse is slain;\n",
      "Will accused: if there is the deed.\n",
      "\n",
      "ISABELLA:\n",
      "Need, die on; but I hear; never 'till.\n",
      "Thus is the rotterner a giild\n",
      "What thus will reach of \n",
      "-----\n",
      "step 3000: train loss 1.6111, val loss 1.7808\n",
      "\n",
      "\n",
      "Second Senator:\n",
      "O, good morrow.\n",
      "\n",
      "\n",
      "Third Musician:\n",
      "Howice, wet perales 'twas my father:\n",
      "But they bid melance occasion and take it.\n",
      "\n",
      "Third Servant:\n",
      "There's at Confes.\n",
      "\n",
      "HENRY BVOLINGEL:\n",
      "Good mad; with me thought to weary grown,\n",
      "Thereof this arrical; on't quickly, go you.\n",
      "\n",
      "FLORIZEL:\n",
      "Not have a gentle Stanley to be so;\n",
      "You are very wore, and shook intinctly\n",
      "gove thee traitors.\n",
      "\n",
      "All:\n",
      "Unless our things are most joy thou art fear\n",
      "Their lights and been may both:\n",
      "Thy daughter is a vilvey.\n",
      "\n",
      "LEOND:\n",
      "O hour\n",
      "\n",
      "-----\n",
      "step 3500: train loss 1.6041, val loss 1.7735\n",
      "\n",
      "A burthen of frayers, boy!\n",
      "It is nothing.\n",
      "\n",
      "LUCIO:\n",
      "More, a officer; head.\n",
      "Why is here ladster were Grumio, this glass of\n",
      "Most disgracious, and fast, to Warwicklord!\n",
      "\n",
      "First Gentleman:\n",
      "What\n",
      "you will accompt Venuto, in what hath it.\n",
      "\n",
      "Thorn:\n",
      "It is a soul: your man is honour; made's son:\n",
      "your ape officence, make me an with tearr:\n",
      "Worthy cravel and you.\n",
      "\n",
      "LEONTES:\n",
      "If you do, Juliet's no,--\n",
      "I give the mastark. Pleasa, that\n",
      "If I see the house; a goodly sanctuary; how\n",
      "haste envying, like to rescuess'd him \n",
      "-----\n",
      "step 4000: train loss 1.5972, val loss 1.8035\n",
      "\n",
      "Indeed, son, in both death, sir,\n",
      "Implant, or usuranness'd Cypeoral.\n",
      "\n",
      "Senator:\n",
      "By this knave I bed, the disposing modedly of\n",
      "your queen's good, to be refore no fearful to\n",
      "When mine: yet did I see the thing.\n",
      "\n",
      "BRAKENBURY:\n",
      "Such a that news she had not left youth\n",
      "To dispatch'd that dead: why, that I do suspect\n",
      "That bears might have done to perceiply,\n",
      "Shrowed my heart in the house and chastily\n",
      "The treessed like a Marcius, butcher\n",
      "Underness'd.\n",
      "\n",
      "GREMIO:\n",
      "Methinks me, what here was my silemnly\n",
      "Together aw\n",
      "-----\n",
      "step 4500: train loss 1.5941, val loss 1.7910\n",
      "\n",
      "I practise too, and proud untimelars\n",
      "Thereo's n the slaughter-houldering shunk\n",
      "His carpe, tide my looks on a boll:\n",
      "This, the sword with them, withal'nt growing thorset\n",
      "Is last to devoise the auth.\n",
      "\n",
      "BRUTH:\n",
      "You for eform the people, not\n",
      "That I spoke it;\n",
      "Of it would yet be absened to join it\n",
      "With one about, and it seems very lour.\n",
      "\n",
      "ONTES:\n",
      "What?\n",
      "\n",
      "MENENIUS:\n",
      "One quarrel? is Suilinet.\n",
      "\n",
      "SICIUS:\n",
      "Peace, Be magabetia?\n",
      "You hath needs o' the only. Is inon,\n",
      "And he that would not make you?\n",
      "Owled men.\n",
      "\n",
      "AUTOLYCU\n",
      "-----\n",
      "step 4999: train loss 1.5832, val loss 1.8080\n",
      "\n",
      "\n",
      "LY:\n",
      "I hope no ease.\n",
      "\n",
      "Servant:\n",
      "Why, my lord, sir.\n",
      "\n",
      "LEONTER:\n",
      "At 'if the keeps.\n",
      "\n",
      "ANOLIO:\n",
      "A very of sedeem Eror can we speak not it\n",
      "In terrely.\n",
      "\n",
      "POLIXENES:\n",
      "A book, whom is now\n",
      "To teem to you?\n",
      "'Tis earth already.\n",
      "\n",
      "AUTOLYCUS:\n",
      "And your honours and all yours,\n",
      "O' Yours about your speech: you this grieves for cots;\n",
      "And you pronounce as I come for them.\n",
      "\n",
      "MASIGS:\n",
      "And much to them be.\n",
      "\n",
      "LADY ANNE:\n",
      "For this shall I have married to a thousand slander,\n",
      "It may think upon you to pardon me.\n",
      "What, dost thou see tha\n",
      "-----\n",
      "\n",
      "Ridegiment and Unto the lass of her wit.\n",
      "\n",
      "Servant:\n",
      "In misch opprisonment after her wrongs,\n",
      "Became our resolution, for her present us\n",
      "That forted giving the tyranny to here:\n",
      "Though she bite absent noble strange;\n",
      "With flowers on hour with posture sanders to thee,\n",
      "And like distrink more and fair brage brothers of sharp,\n",
      "He shiden ye'll come at with our hotesth shades best,\n",
      "That in the designs of his oath wrong hide.\n",
      "\n",
      "COMINIUS:\n",
      "If, you should by that comke before\n",
      "Eason from the honours of your prais\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "batch_size = 64 # how many independent sequences will we process in parallel?\n",
    "context_size = 256 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 384\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.2\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "##! new\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size=head_size, context_size=context_size,n_embd=n_embd,dropout=dropout) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "##! new\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "##! new\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class GPTLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(context_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "        # better init, not covered in the original GPT video, but important, will cover in followup video\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module): #!! \n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        idx = idx.to(device)\n",
    "        if targets is not None:\n",
    "            targets = targets.to(device)\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last context_size tokens\n",
    "            idx_cond = idx[:, -context_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "model = GPTLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss(m)\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")      \n",
    "        context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "        print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n",
    "        print('-----')\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train', context_size)\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ссылки\n",
    "\n",
    "1. [Neural Networks: Zero to Hero](https://www.youtube.com/watch?v=VMj-3S1tku0&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&index=1&t=0s). Лучшее, что вы можете посмотреть, чтобы полноценно вкатиться нейросети и докатиться до понимания того, как под капотом работает ChatGPT\n",
    "2. [Transformer, explained in detail](https://youtu.be/iOrNbK2T92M?feature=shared&t=18) хорошая русскоязычная лекция про трансформеры\n",
    "3. Честное [объяснение](https://towardsdatascience.com/master-positional-encoding-part-i-63c05d90a0c3) тригонометрических позиционных энкодингов. Требуется VPN, если нет, то копия в pdf-формате лежит [рядом](Master%20Positional%20Encoding_%20Part%20I%20_%20by%20Jonathan%20Kernes%20_%20Towards%20Data%20Science.pdf)\n",
    "4. [Имплементация GPT-2](https://github.com/karpathy/nanoGPT) в 300 строк питона\n",
    "5. Хорошая статья про [Attention](https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html)\n",
    "6. [Аттеншен и трансформер в картинках](https://jalammar.github.io/illustrated-transformer/) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
